(window.webpackJsonp=window.webpackJsonp||[]).push([[68],{725:function(t,s,a){"use strict";a.r(s);var n=a(42),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"深度学习笔记"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#深度学习笔记"}},[t._v("#")]),t._v(" 深度学习笔记")]),t._v(" "),s("h2",{attrs:{id:"一个最基础的例子"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一个最基础的例子"}},[t._v("#")]),t._v(" 一个最基础的例子")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("l0 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("units"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" input_shape"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("l0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean_squared_error'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimizers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nhistory "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("celsius_q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fahrenheit_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br")])]),s("h2",{attrs:{id:"术语"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#术语"}},[t._v("#")]),t._v(" 术语：")]),t._v(" "),s("ul",[s("li",[t._v("**特征：**模型的输入")]),t._v(" "),s("li",[t._v("**样本：**用于训练流程的输入/输出对")]),t._v(" "),s("li",[t._v("**标签：**模型的输出")]),t._v(" "),s("li",[t._v("**层级：**神经网络中相互连接的节点集合。")]),t._v(" "),s("li",[t._v("**模型：**神经网络的表示法")]),t._v(" "),s("li",[t._v("**密集全连接层 (FC)：**一个层级中的每个节点都与上个层级中的每个节点相连。")]),t._v(" "),s("li",[t._v("**权重和偏差：**模型的内部变量")]),t._v(" "),s("li",[t._v("**损失：**期望输出和真实输出之间的差值")]),t._v(" "),s("li",[t._v("**MSE：**均方误差，一种损失函数，它会将一小部分很大的差值视作比大量很小的差值更糟糕。")]),t._v(" "),s("li",[t._v("**梯度下降法：**每次小幅调整内部变量，从而逐渐降低损失函数的算法。")]),t._v(" "),s("li",[t._v("**优化器：**梯度下降法的一种具体实现方法。（有很多算法。在这门课程中，我们将仅使用“Adam”优化器，它是 "),s("em",[t._v("ADAptive with Momentum")]),t._v(" 的简称，并且被视为最佳优化器。）")]),t._v(" "),s("li",[t._v("**学习速率：**梯度下降过程中的损失改进“步长”。")]),t._v(" "),s("li",[t._v("**批次：**在训练神经网络的过程中使用的一组样本。")]),t._v(" "),s("li",[t._v("**周期：**完全经过整个训练数据集一轮")]),t._v(" "),s("li",[t._v("**前向传播：**根据输入计算输出值")]),t._v(" "),s("li",[t._v("**反向传播：**根据优化器算法计算内部变量的调整幅度，从输出层级开始，并往回计算每个层级，直到抵达输入层。")]),t._v(" "),s("li",[t._v("**扁平化：**将二维图像转换为一维向量的过程")]),t._v(" "),s("li",[t._v("**ReLU：**一种激活函数，使模型能够解决非线性问题")]),t._v(" "),s("li",[t._v("**Softmax：**一种函数，能够为每个潜在输出类别生成概率")]),t._v(" "),s("li",[t._v("**分类：**一种机器学习模型，用于区分两个或多个输出类别")])])])}),[],!1,null,null,null);s.default=e.exports}}]);